<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>cuMl · RAPIDS.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://tylerjthomas9.github.io/RAPIDS.jl/cuml/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">RAPIDS.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../python_api/">Python API</a></li><li class="is-active"><a class="tocitem" href>cuMl</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#MLJ-Example-Classification"><span>MLJ Example - Classification</span></a></li><li class="toplevel"><a class="tocitem" href="#MLJ-Example-Regression"><span>MLJ Example - Regression</span></a></li><li><a class="tocitem" href="#Clustering"><span>Clustering</span></a></li><li><a class="tocitem" href="#Classification"><span>Classification</span></a></li><li><a class="tocitem" href="#Regression"><span>Regression</span></a></li><li><a class="tocitem" href="#Dimensionality-Reduction"><span>Dimensionality Reduction</span></a></li><li><a class="tocitem" href="#Time-Series"><span>Time Series</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>cuMl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>cuMl</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/tylerjthomas9/RAPIDS.jl.git" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="MLJ-API"><a class="docs-heading-anchor" href="#MLJ-API">MLJ API</a><a id="MLJ-API-1"></a><a class="docs-heading-anchor-permalink" href="#MLJ-API" title="Permalink"></a></h1><h1 id="MLJ-Example-Classification"><a class="docs-heading-anchor" href="#MLJ-Example-Classification">MLJ Example - Classification</a><a id="MLJ-Example-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#MLJ-Example-Classification" title="Permalink"></a></h1><pre><code class="language-julia hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = LogisticRegression()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre><h1 id="MLJ-Example-Regression"><a class="docs-heading-anchor" href="#MLJ-Example-Regression">MLJ Example - Regression</a><a id="MLJ-Example-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#MLJ-Example-Regression" title="Permalink"></a></h1><pre><code class="language-julia hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = LinearRegression()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre><h2 id="Clustering"><a class="docs-heading-anchor" href="#Clustering">Clustering</a><a id="Clustering-1"></a><a class="docs-heading-anchor-permalink" href="#Clustering" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.KMeans" href="#RAPIDS.KMeans"><code>RAPIDS.KMeans</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KMeans</code></pre><p>A model type for constructing a k means, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Clustering Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">KMeans = @load KMeans pkg=cuML Clustering Methods</code></pre><p>Do <code>model = KMeans()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>KMeans(n_clusters=...)</code>.</p><p><code>KMeans</code> is a wrapper for the RAPIDS KMeans Clustering.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>n_clusters=8</code>: The number of clusters/centroids.</li><li><code>max_iter=300</code>: Maximum iterations of the EM algorithm. </li><li><code>tol=1e-4</code>: Stopping criterion when centroid means do not change much.</li><li><code>random_state=1</code>: Seed for the random number generator.</li><li><code>init=&quot;scalable-k-means++&quot;</code><ul><li><code>scalable-k-means++</code> or <code>k-means||</code>: Uses fast and stable scalable kmeans++ initialization.</li><li><code>random</code>: Choose <code>n_cluster</code> observations (rows) at random from data for the initial centroids.</li></ul></li><li><code>n_init=1</code>: Number of instances the k-means algorithm will be called with different seeds. The final results will be from the instance that produces lowest inertia out of n_init instances.</li><li><code>oversampling_factor=20</code>: The amount of points to sample in scalable k-means++ initialization for potential centroids.</li><li><code>max_samples_per_batch=32768</code>: The number of data samples to use for batches of the pairwise distance computation.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features</code>: the names of the features encountered in training.</p></li><li><p><code>labels</code>: Vector of observation labels. </p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = KMeans()
mach = machine(model, X)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.DBSCAN" href="#RAPIDS.DBSCAN"><code>RAPIDS.DBSCAN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DBSCAN</code></pre><p>A model type for constructing a dbscan, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Clustering Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">DBSCAN = @load DBSCAN pkg=cuML Clustering Methods</code></pre><p>Do <code>model = DBSCAN()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>DBSCAN(eps=...)</code>.</p><p><code>DBSCAN</code> is a wrapper for the RAPIDS DBSCAN Clustering.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>eps=0.5</code>: The maximum distance between 2 points such they reside in the same neighborhood.</li><li><code>min_samples=5</code>: The number of samples in a neighborhood such that this group can be considered as an important core point (including the point itself).</li><li><code>metric=&quot;euclidean</code>: The metric to use when calculating distances between points.<ul><li><code>euclidean</code>, <code>cosine</code>, <code>precomputed</code></li></ul></li><li><code>max_mbytes_per_batch=nothing</code>: Calculate batch size using no more than this number of megabytes for the pairwise distance computation. </li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features</code>: the names of the features encountered in training.</p></li><li><p><code>labels</code>: Vector of observation labels. </p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = DBSCAN()
mach = machine(model, X)
fit!(mach)
preds = mach.report.labels #DBSCAN does not have a predict method</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.AgglomerativeClustering" href="#RAPIDS.AgglomerativeClustering"><code>RAPIDS.AgglomerativeClustering</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AgglomerativeClustering</code></pre><p>A model type for constructing a agglomerative clustering, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Clustering Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">AgglomerativeClustering = @load AgglomerativeClustering pkg=cuML Clustering Methods</code></pre><p>Do <code>model = AgglomerativeClustering()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>AgglomerativeClustering(verbose=...)</code>.</p><p><code>AgglomerativeClustering</code> is a wrapper for the RAPIDS Agglomerative Clustering.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>n_clusters=8</code>: The number of clusters.</li><li><code>affinity=&quot;euclidean&quot;</code>: Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, or “cosine”. </li><li><code>n_neighbors=15</code>: The number of neighbors to compute when <code>connectivity = “knn”</code></li><li><code>connectivity=&quot;knn&quot;</code>:<ul><li><code>knn</code> will sparsify the fully-connected connectivity matrix to save memory and enable much larger inputs.</li><li><code>pairwise</code> will compute the entire fully-connected graph of pairwise distances between each set of points.</li></ul></li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features</code>: the names of the features encountered in training.</p></li><li><p><code>labels</code>: Vector of observation labels. </p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = AgglomerativeClustering()
mach = machine(model, X)
fit!(mach)
preds = mach.report.labels #AgglomerativeClustering does not have a predict method</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.HDBSCAN" href="#RAPIDS.HDBSCAN"><code>RAPIDS.HDBSCAN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HDBSCAN</code></pre><p>A model type for constructing a hdbscan, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Clustering Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">HDBSCAN = @load HDBSCAN pkg=cuML Clustering Methods</code></pre><p>Do <code>model = HDBSCAN()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>HDBSCAN(alpha=...)</code>.</p><p><code>HDBSCAN</code> is a wrapper for the RAPIDS HDBSCAN Clustering.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>alpha=1.0</code>: A distance scaling parameter as used in robust single linkage.</li><li><code>min_cluster_size=5</code>: The minimum number of samples in a group for that group to be considered a cluster.</li><li><code>min_samples=nothing</code>: The number of samples in a neighborhood for a point to be considered as a core point.</li><li><code>cluster_selection_epsilon=0.0</code>: A distance threshold. Clusters below this value will be merged.</li><li><code>max_cluster_size=0</code>: A limit to the size of clusters returned by the eom algorithm.</li><li><code>p=2</code>: p value to use if using the minkowski metric.</li><li><code>cluster_selection_method=&quot;eom&quot;</code>: The method used to select clusters from the condensed tree. <code>eom</code>/<code>leaf</code></li><li><code>allow_single_cluster=false</code>: Allow <code>HDBSCAN</code> to produce a single cluster.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features</code>: the names of the features encountered in training.</p></li><li><p><code>labels</code>: Vector of observation labels. </p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = HDBSCAN()
mach = machine(model, X)
fit!(mach)
preds = mach.report.labels #HDBSCAN does not have a predict method</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><h2 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.LogisticRegression" href="#RAPIDS.LogisticRegression"><code>RAPIDS.LogisticRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogisticRegression</code></pre><p>A model type for constructing a logistic regression, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Classification Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">LogisticRegression = @load LogisticRegression pkg=cuML Classification Methods</code></pre><p>Do <code>model = LogisticRegression()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>LogisticRegression(penalty=...)</code>.</p><p><code>LogisticRegression</code> is a wrapper for the RAPIDS Logistic Regression.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><p><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></p></li><li><p><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element   scitype is <code>&lt;:OrderedFactor</code> or <code>&lt;:Multiclass</code>; check the scitype   with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>penalty=&quot;l2&quot;</code>: Normalization/penalty function (&quot;none&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;elasticnet&quot;).<ul><li><code>none</code>: the L-BFGS solver will be used</li><li><code>l1</code>: The L1 penalty is best when there are only a few useful features (sparse), and you       want to zero out non-important features. The L-BFGS solver will be used.</li><li><code>l2</code>: The L2 penalty is best when you have a lot of important features, especially if they       are correlated.The L-BFGS solver will be used.</li><li><code>elasticnet</code>: A combination of the L1 and L2 penalties. The OWL-QN solver will be used if               <code>l1_ratio&gt;0</code>, otherwise the L-BFGS solver will be used.</li></ul></li><li>`tol=1e-4&#39;: Tolerance for stopping criteria. </li><li><code>C=1.0</code>: Inverse of regularization strength.</li><li><code>fit_intercept=true</code>: If True, the model tries to correct for the global mean of y.                        If False, the model expects that you have centered the data.</li><li><code>class_weight=&quot;balanced&quot;</code>: Dictionary or <code>&quot;balanced&quot;</code>.</li><li><code>max_iter=1000</code>: Maximum number of iterations taken for the solvers to converge.</li><li><code>linesearch_max_iter=50</code>: Max number of linesearch iterations per outer iteration used in                            the lbfgs and owl QN solvers.</li><li><code>solver=&quot;qn&quot;</code>: Algorithm to use in the optimization problem. Currently only <code>qn</code> is                supported, which automatically selects either <code>L-BFGS</code>or <code>OWL-QN</code></li><li><code>l1_ratio=nothing</code>: The Elastic-Net mixing parameter. </li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </p></li><li><p><code>predict_proba(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are probabilistic, but uncalibrated.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>classes_seen</code>: list of target classes actually observed in training</p></li><li><p><code>features</code>: the names of the features encountered in training.</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = LogisticRegression()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.MBSGDClassifier" href="#RAPIDS.MBSGDClassifier"><code>RAPIDS.MBSGDClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MBSGDClassifier</code></pre><p>A model type for constructing a mbsgd classifier, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Classification Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">MBSGDClassifier = @load MBSGDClassifier pkg=cuML Classification Methods</code></pre><p>Do <code>model = MBSGDClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>MBSGDClassifier(loss=...)</code>.</p><p><code>MBSGDClassifier</code> is a wrapper for the RAPIDS Mini Batch SGD Classifier.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><p><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></p></li><li><p><code>y</code>: is an <code>AbstractVector</code> finite target.</p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>loss=&quot;squared_loss&quot;</code>: Loss function (&quot;hinge&quot;, &quot;log&quot;, &quot;squared_loss&quot;).<ul><li><code>hinge</code>: Linear SVM</li><li><code>log</code>: Logistic regression</li><li><code>squared_loss</code>: Linear regression</li></ul></li><li><code>penalty=&quot;none&quot;</code>: Normalization/penalty function (&quot;none&quot;, &quot;l1&quot;, &quot;l2&quot;, &quot;elasticnet&quot;).<ul><li><code>none</code>: the L-BFGS solver will be used</li><li><code>l1</code>: The L1 penalty is best when there are only a few useful features (sparse), and you       want to zero out non-important features. The L-BFGS solver will be used.</li><li><code>l2</code>: The L2 penalty is best when you have a lot of important features, especially if they       are correlated.The L-BFGS solver will be used.</li><li><code>elasticnet</code>: A combination of the L1 and L2 penalties. The OWL-QN solver will be used if               <code>l1_ratio&gt;0</code>, otherwise the L-BFGS solver will be used.</li></ul></li><li><code>alpha=1e-4</code>: The constant value which decides the degree of regularization.</li><li><code>l1_ratio=nothing</code>: The Elastic-Net mixing parameter. </li><li><code>batch_size</code>: The number of samples in each batch.</li><li><code>fit_intercept=true</code>: If True, the model tries to correct for the global mean of y.                        If False, the model expects that you have centered the data.</li><li><code>epochs=1000</code>: The number of times the model should iterate through the entire dataset during training.</li><li>`tol=1e-3&#39;: The training process will stop if current<em>loss &gt; previous</em>loss - tol.</li><li><code>shuffle=true</code>: If true, shuffles the training data after each epoch.</li><li><code>eta0=1e-3</code>: The initial learning rate.</li><li><code>power_t=0.5</code>: The exponent used for calculating the invscaling learning rate.</li><li><code>learning_rate=&quot;constant</code>: Method for modifying the learning rate during training                           (&quot;adaptive&quot;, &quot;constant&quot;, &quot;invscaling&quot;, &quot;optimal&quot;)<ul><li><code>optimal</code>: not supported</li><li><code>constant</code>: constant learning rate</li><li><code>adaptive</code>: changes the learning rate if the training loss or the validation accuracy does               not improve for n<em>iter</em>no_change epochs. The old learning rate is generally divided by 5.</li><li><code>invscaling</code>: <code>eta = eta0 / pow(t, power_t)</code></li></ul></li><li><code>n_iter_no_change=5</code>: the number of epochs to train without any imporvement in the model</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </p></li><li><p><code>predict_proba(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are probabilistic, but uncalibrated.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>classes_seen</code>: list of target classes actually observed in training</p></li><li><p><code>features</code>: the names of the features encountered in training.</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = MBSGDClassifier()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.RandomForestClassifier" href="#RAPIDS.RandomForestClassifier"><code>RAPIDS.RandomForestClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomForestClassifier</code></pre><p>A model type for constructing a random forest classifier, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Classification Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">RandomForestClassifier = @load RandomForestClassifier pkg=cuML Classification Methods</code></pre><p>Do <code>model = RandomForestClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>RandomForestClassifier(n_estimators=...)</code>.</p><p><code>RandomForestClassifier</code> is a wrapper for the RAPIDS RandomForestClassifier.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> finite target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_estimators=100</code>: The total number of trees in the forest.</li><li><code>split_creation=2</code>: The criterion used to split nodes<ul><li><code>0</code> or <code>gini</code> for gini impurity</li><li><code>1</code> or <code>entropy</code> for information gain (entropy)</li></ul></li><li><code>bootstrap=true</code>: If true, each tree in the forest is built using a bootstrap sample with replacement.</li><li><code>max_samples=1.0</code>: Ratio of dataset rows used while fitting each tree.</li><li><code>max_depth=16</code>: Maximum tree depth.</li><li><code>max_leaves=-1</code>: Maximum leaf nodes per tree. Soft constraint. Unlimited, If <code>-1</code>.</li><li><code>max_features=&quot;auto&quot;</code>: Ratio of number of features (columns) to consider per node split.<ul><li>If type <code>Int</code> then max_features is the absolute count of features to be used.</li><li>If type <code>Float64</code> then <code>max_features</code> is a fraction.</li><li>If <code>auto</code> then <code>max_features=n_features = 1.0</code>.</li><li>If <code>sqrt</code> then <code>max_features=1/sqrt(n_features)</code>.</li><li>If <code>log2</code> then <code>max_features=log2(n_features)/n_features</code>.</li><li>If None, then <code>max_features=1.0</code>.</li></ul></li><li><code>n_bins=128</code>: Maximum number of bins used by the split algorithm per feature.</li><li><code>n_streams=4</code>: Number of parallel streams used for forest building</li><li><code>min_samples_leaf=1</code>: The minimum number of samples in each leaf node.<ul><li>If type <code>Int</code>, then <code>min_samples_leaf</code> represents the minimum number.</li><li>If <code>Float64</code>, then <code>min_samples_leaf</code> represents a fraction and <code>ceil(min_samples_leaf * n_rows)</code>   is the minimum number of samples for each leaf node.</li></ul></li><li><code>min_samples_split=2</code>: The minimum number of samples required to split an internal node.<ul><li>If type <code>Int</code>, then <code>min_samples_split</code> represents the minimum number.</li><li>If <code>Float64</code>, then <code>min_samples_split</code> represents a fraction and <code>ceil(min_samples_leaf * n_rows)</code>   is the minimum number of samples for each leaf node.</li></ul></li><li><code>min_impurity_decrease=0.0</code>: The minimum decrease in impurity required for node to be split.</li><li><code>max_batch_size=4096</code>: Maximum number of nodes that can be processed in a given batch.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>classes_seen</code>: list of target classes actually observed in training</p></li><li><p><code>features</code>: the names of the features encountered in training.</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = RandomForestClassifier()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.SVC" href="#RAPIDS.SVC"><code>RAPIDS.SVC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SVC</code></pre><p>A model type for constructing a svc, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Classification Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">SVC = @load SVC pkg=cuML Classification Methods</code></pre><p>Do <code>model = SVC()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>SVC(C=...)</code>.</p><p><code>SVC</code> is a wrapper for the RAPIDS SVC.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><p><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></p></li><li><p><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element   scitype is <code>&lt;:OrderedFactor</code> or <code>&lt;:Multiclass</code>; check the scitype   with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>C=1.0</code>: The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.</li><li><code>kernel=&quot;rbf&quot;</code>: <code>linear</code>, <code>poly</code>, <code>rbf</code>, <code>sigmoid</code> are supported.</li><li><code>degree=3</code>: Degree of polynomial kernel function.</li><li><code>gamma=&quot;scale&quot;</code><ul><li><code>auto</code>: gamma will be set to <code>1 / n_features</code></li><li><code>scale</code>: gamma will be set to <code>1 / (n_features * var(X))</code></li></ul></li><li><code>coef0=0.0</code>: Independent term in kernel function, only signifficant for poly and sigmoid.</li><li><code>tol=0.001</code>: Tolerance for stopping criterion.</li><li><code>cache_size=1024.0</code>: Size of the cache during training in MiB.</li><li><code>class_weight=nothing</code>: Weights to modify the parameter C for class i to <code>class_weight[i]*C</code>. The string <code>&quot;balanced&quot;</code>` is also accepted.</li><li><code>max_iter=-1</code>: Limit the number of outer iterations in the solver. If <code>-1</code> (default) then <code>max_iter=100*n_samples</code>.</li><li><code>multiclass_strategy=&quot;ovo&quot;</code><ul><li><code>ovo</code>: OneVsOneClassifier</li><li><code>ovr</code>: OneVsRestClassifier</li></ul></li><li><code>nochange_steps=1000</code>: Stop training if a <code>1e-3*tol</code> difference isn&#39;t seen in <code>nochange_steps</code> steps.</li><li><code>probability=false</code>: Enable or disable probability estimates.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </p></li><li><p><code>predict_proba(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are probabilistic, but uncalibrated.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>classes_seen</code>: list of target classes actually observed in training</p></li><li><p><code>features</code>: the names of the features encountered in training.</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = SVC()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.LinearSVC" href="#RAPIDS.LinearSVC"><code>RAPIDS.LinearSVC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearSVC</code></pre><p>A model type for constructing a linear svc, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Classification Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">LinearSVC = @load LinearSVC pkg=cuML Classification Methods</code></pre><p>Do <code>model = LinearSVC()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>LinearSVC(penalty=...)</code>.</p><p><code>LinearSVC</code> is a wrapper for the RAPIDS LinearSVC.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><p><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></p></li><li><p><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element   scitype is <code>&lt;:OrderedFactor</code> or <code>&lt;:Multiclass</code>; check the scitype   with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>penalty=&quot;l2</code>: <code>l1</code> (Lasso) or <code>l2</code> (Ridge) penalty.</li><li><code>loss=&quot;squared_hinge&quot;</code>: The loss term of the target function.</li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>penalized_intercept=true</code>: When true, the bias term is treated the same way as other features.</li><li><code>max_iter=1000</code>: Maximum number of iterations for the underlying solver.</li><li><code>linesearch_max_iter=1000</code>: Maximum number of linesearch (inner loop) iterations for the underlying (QN) solver.</li><li><code>lbfgs_memory=5</code>: Number of vectors approximating the hessian for the underlying QN solver (l-bfgs).</li><li><code>C=1.0</code>: The constant scaling factor of the loss term in the target formula <code>F(X, y) = penalty(X) + C * loss(X, y)</code>.</li><li><code>grad_tol=0.0001</code>: The threshold on the gradient for the underlying QN solver.</li><li><code>change_tol=0.00001</code>: The threshold on the function change for the underlying QN solver.</li><li><code>tol=nothing</code>: Tolerance for stopping criterion.</li><li><code>probabability=false</code>: Enable or disable probability estimates.</li><li><code>multi_class=&quot;ovo&quot;</code><ul><li><code>ovo</code>: OneVsOneClassifier</li><li><code>ovr</code>: OneVsRestClassifier</li></ul></li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </p></li><li><p><code>predict_proba(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are probabilistic, but uncalibrated.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>classes_seen</code>: list of target classes actually observed in training</p></li><li><p><code>features</code>: the names of the features encountered in training.</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = LinearSVC()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.KNeighborsClassifier" href="#RAPIDS.KNeighborsClassifier"><code>RAPIDS.KNeighborsClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KNeighborsClassifier</code></pre><p>A model type for constructing a k neighbors classifier, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Classification Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">KNeighborsClassifier = @load KNeighborsClassifier pkg=cuML Classification Methods</code></pre><p>Do <code>model = KNeighborsClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>KNeighborsClassifier(algorithm=...)</code>.</p><p><code>KNeighborsClassifier</code> is a wrapper for the RAPIDS K-Nearest Neighbors Classifier.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><p><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></p></li><li><p><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element   scitype is <code>&lt;:OrderedFactor</code> or <code>&lt;:Multiclass</code>; check the scitype   with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_neighbors=5</code>: Default number of neighbors to query.</li><li><code>algorithm=&quot;brute&quot;</code>: Only one algorithm is currently supported.</li><li><code>metric=&quot;euclidean&quot;</code>: Distance metric to use.</li><li><code>weights=&quot;uniform&quot;</code>: Sample weights to use. Currently, only the uniform strategy is supported.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </p></li><li><p><code>predict_proba(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are probabilistic, but uncalibrated.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>classes_seen</code>: list of target classes actually observed in training</p></li><li><p><code>features</code>: the names of the features encountered in training.</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = KNeighborsClassifier()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><h2 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.LinearRegression" href="#RAPIDS.LinearRegression"><code>RAPIDS.LinearRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearRegression</code></pre><p>A model type for constructing a linear regression, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">LinearRegression = @load LinearRegression pkg=cuML Regression Methods</code></pre><p>Do <code>model = LinearRegression()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>LinearRegression(algorithm=...)</code>.</p><p><code>LinearRegression</code> is a wrapper for the RAPIDS Linear Regression.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>algorithm=&quot;eig&quot;</code>: <ul><li><code>eig</code>: use an eigendecomposition of the covariance matrix.</li><li><code>qr</code>: use QR decomposition algorithm and solve <code>Rx = Q^T y</code></li><li><code>svd</code>: alias for svd-jacobi.</li><li><code>svd-qr</code>: compute SVD decomposition using QR algorithm.</li><li><code>svd-jacobi</code>: compute SVD decomposition using Jacobi iterations.</li></ul></li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>normalize=true</code>: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation.                    If false, no scaling will be done. </li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = LinearRegression()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.Ridge" href="#RAPIDS.Ridge"><code>RAPIDS.Ridge</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ridge</code></pre><p>A model type for constructing a ridge, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">Ridge = @load Ridge pkg=cuML Regression Methods</code></pre><p>Do <code>model = Ridge()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>Ridge(alpha=...)</code>.</p><p><code>Ridge</code> is a wrapper for the RAPIDS Ridge Regression.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>alpha=1.0</code>: Regularization strength - must be a positive float. Larger values specify stronger regularization.</li><li><code>solver=&quot;eig&quot;</code>: <ul><li><code>cd</code>: use coordinate descent. Very fast and is suitable for large problems.</li><li><code>eig</code>: use an eigendecomposition of the covariance matrix.</li><li><code>svd</code>: alias for svd-jacobi. Slower, but guaranteed to be stable.</li></ul></li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>normalize=true</code>: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation.                    If false, no scaling will be done. </li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = Ridge()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.Lasso" href="#RAPIDS.Lasso"><code>RAPIDS.Lasso</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lasso</code></pre><p>A model type for constructing a lasso, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">Lasso = @load Lasso pkg=cuML Regression Methods</code></pre><p>Do <code>model = Lasso()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>Lasso(alpha=...)</code>.</p><p><code>Lasso</code> is a wrapper for the RAPIDS Lasso Regression.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>alpha=1.0</code>: Constant that multiplies the L1 term. alpha = 0 is equivalent to an ordinary least square.</li><li><code>tol=1e-4</code>: Tolerance for stopping criteria. </li><li><code>max_iter=1000</code>: Maximum number of iterations taken for the solvers to converge.</li><li><code>solver=&quot;cd&quot;</code>: <ul><li><code>cd</code>: Coordinate descent.</li><li><code>qn</code>: quasi-newton. You may find the alternative ‘qn’ algorithm is faster when the number of features is sufficiently large, but the sample size is small.</li></ul></li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>normalize=true</code>: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation.                    If false, no scaling will be done. </li><li><code>selection=&quot;cyclic&quot;</code>: <ul><li><code>cyclic</code>: loop over features to update coefficients.</li><li><code>random</code>: a random coefficient is updated every iteration.</li></ul>a random coefficient is updated every iteration rather than looping over features sequentially by default.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = Lasso()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.ElasticNet" href="#RAPIDS.ElasticNet"><code>RAPIDS.ElasticNet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ElasticNet</code></pre><p>A model type for constructing a elastic net, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">ElasticNet = @load ElasticNet pkg=cuML Regression Methods</code></pre><p>Do <code>model = ElasticNet()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ElasticNet(alpha=...)</code>.</p><p><code>ElasticNet</code> is a wrapper for the RAPIDS ElasticNet Regression.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>alpha=1.0</code>: Constant that multiplies the L1 term. alpha = 0 is equivalent to an ordinary least square.</li><li><code>l1_ratio=0.5</code>: The ElasticNet mixing parameter.</li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>normalize=true</code>: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation. </li><li><code>max_iter=1000</code>: Maximum number of iterations taken for the solvers to converge.</li><li><code>tol=1e-3</code>: Tolerance for stopping criteria. </li><li><code>solver=&quot;cd&quot;</code>: <ul><li><code>cd</code>: Coordinate descent.</li><li><code>qn</code>: quasi-newton. You may find the alternative ‘qn’ algorithm is faster when the number of features is sufficiently large, but the sample size is small.</li></ul></li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>selection=&quot;cyclic&quot;</code>: <ul><li><code>cyclic</code>: loop over features to update coefficients.</li><li><code>random</code>: a random coefficient is updated every iteration.</li></ul>a random coefficient is updated every iteration rather than looping over features sequentially by default.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = ElasticNet()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.MBSGDRegressor" href="#RAPIDS.MBSGDRegressor"><code>RAPIDS.MBSGDRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MBSGDRegressor</code></pre><p>A model type for constructing a mbsgd regressor, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">MBSGDRegressor = @load MBSGDRegressor pkg=cuML Regression Methods</code></pre><p>Do <code>model = MBSGDRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>MBSGDRegressor(loss=...)</code>.</p><p><code>MBSGDRegressor</code> is a wrapper for the RAPIDS MBSGDRegressor.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>loss=&quot;squared_loss</code>: <code>squared_loss</code> uses linear regression</li><li><code>penalty=&quot;none</code>: <ul><li><code>none</code>: No regularization.</li><li><code>l1</code>: L1 norm (Lasso).</li><li><code>l2</code>: L2 norm (Ridge).</li><li><code>elasticnet</code>: weighted average of L1 and L2 norms.</li></ul></li><li><code>alpha=0.0001</code>: The constant value which decides the degree of regularization.</li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>l1_ratio=0.5</code>: The ElasticNet mixing parameter.</li><li><code>batch_size=32</code>: The number of samples in each batch.</li><li><code>epochs=1000</code>: The number of times the model should iterate through the entire dataset.</li><li><code>tol=1e-3</code>: Tolerance for stopping criteria.</li><li><code>shuffle=true</code>: True, shuffles the training data after each epoch False, does not shuffle the training data after each epoch</li><li><code>eta_0=0.001</code>: Initial learning rate.</li><li><code>power_t=0.5</code>: The exponent used for calculating the invscaling learning rate.</li><li><code>learning_rate=&quot;constant</code>:</li><li><code>n_iter_no_change=5</code>: The number of epochs to train without any improvement in the model.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = MBSGDRegressor()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.RandomForestRegressor" href="#RAPIDS.RandomForestRegressor"><code>RAPIDS.RandomForestRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomForestRegressor</code></pre><p>A model type for constructing a random forest regressor, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">RandomForestRegressor = @load RandomForestRegressor pkg=cuML Regression Methods</code></pre><p>Do <code>model = RandomForestRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>RandomForestRegressor(n_estimators=...)</code>.</p><p><code>RandomForestRegressor</code> is a wrapper for the RAPIDS RandomForestRegressor.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_estimators=100</code>: The total number of trees in the forest.</li><li><code>split_creation=2</code>: The criterion used to split nodes<ul><li><code>2</code> or <code>mse</code> for mean squared error</li><li><code>4</code> or <code>poisson</code> for poisson half deviance</li><li><code>5</code> or <code>gamma</code> for gamma half deviance</li><li><code>6</code> or <code>inverse_gaussian</code> for inverse gaussian deviance</li></ul></li><li><code>bootstrap=true</code>: If true, each tree in the forest is built using a bootstrap sample with replacement.</li><li><code>max_samples=1.0</code>: Ratio of dataset rows used while fitting each tree.</li><li><code>max_depth=16</code>: Maximum tree depth.</li><li><code>max_leaves=-1</code>: Maximum leaf nodes per tree. Soft constraint. Unlimited, If <code>-1</code>.</li><li><code>max_features=&quot;auto&quot;</code>: Ratio of number of features (columns) to consider per node split.<ul><li>If type <code>Int</code> then max_features is the absolute count of features to be used.</li><li>If type <code>Float64</code> then <code>max_features</code> is a fraction.</li><li>If <code>auto</code> then <code>max_features=n_features = 1.0</code>.</li><li>If <code>sqrt</code> then <code>max_features=1/sqrt(n_features)</code>.</li><li>If <code>log2</code> then <code>max_features=log2(n_features)/n_features</code>.</li><li>If None, then <code>max_features=1.0</code>.</li></ul></li><li><code>n_bins=128</code>: Maximum number of bins used by the split algorithm per feature.</li><li><code>n_streams=4</code>: Number of parallel streams used for forest building</li><li><code>min_samples_leaf=1</code>: The minimum number of samples in each leaf node.<ul><li>If type <code>Int</code>, then <code>min_samples_leaf</code> represents the minimum number.</li><li>If <code>Float64</code>, then <code>min_samples_leaf</code> represents a fraction and <code>ceil(min_samples_leaf * n_rows)</code>   is the minimum number of samples for each leaf node.</li></ul></li><li><code>min_samples_split=2</code>: The minimum number of samples required to split an internal node.<ul><li>If type <code>Int</code>, then <code>min_samples_split</code> represents the minimum number.</li><li>If <code>Float64</code>, then <code>min_samples_split</code> represents a fraction and <code>ceil(min_samples_leaf * n_rows)</code>   is the minimum number of samples for each leaf node.</li></ul></li><li><code>min_impurity_decrease=0.0</code>: The minimum decrease in impurity required for node to be split.</li><li><code>accuracy_metric=&quot;r2&quot;</code><ul><li><code>r2</code>: r-squared</li><li><code>median_ae</code>: median of absolute error</li><li><code>mean_ae</code>: mean of absolute error</li><li><code>mse</code>: mean squared error</li></ul></li><li><code>max_batch_size=4096</code>: Maximum number of nodes that can be processed in a given batch.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training., in an order consistent with the output of <code>print_tree</code> (see below)</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = RandomForestRegressor()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.CD" href="#RAPIDS.CD"><code>RAPIDS.CD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CD</code></pre><p>A model type for constructing a cd, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">CD = @load CD pkg=cuML Regression Methods</code></pre><p>Do <code>model = CD()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>CD(loss=...)</code>.</p><p><code>CD</code> is a wrapper for the RAPIDS Coordinate Descent.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>alpha=0.0001</code>: Regularization strength. alpha = 0 is equivalent to an ordinary least square.</li><li><code>l1_ratio=0.15</code>: The ElasticNet mixing parameter.</li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>normalize=false</code>: If true, the data is normalized.</li><li><code>tol=0.001</code>: The tolerance for the optimization: if the updates are smaller than tol, solver stops.</li><li><code>shuffle=true</code>: If true, a random coefficient is updated at each iteration. </li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = CD()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.SVR" href="#RAPIDS.SVR"><code>RAPIDS.SVR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SVR</code></pre><p>A model type for constructing a svr, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">SVR = @load SVR pkg=cuML Regression Methods</code></pre><p>Do <code>model = SVR()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>SVR(C=...)</code>.</p><p><code>SVR</code> is a wrapper for the RAPIDS SVM Regressor.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>C=1.0</code>: The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.</li><li><code>kernel=&quot;rbf&quot;</code>: <code>linear</code>, <code>poly</code>, <code>rbf</code>, <code>sigmoid</code> are supported.</li><li><code>degree=3</code>: Degree of polynomial kernel function.</li><li><code>gamma=&quot;scale&quot;</code><ul><li><code>auto</code>: gamma will be set to <code>1 / n_features</code></li><li><code>scale</code>: gamma will be set to <code>1 / (n_features * var(X))</code></li></ul></li><li><code>coef0=0.0</code>: Independent term in kernel function, only signifficant for poly and sigmoid.</li><li><code>tol=0.001</code>: Tolerance for stopping criterion.</li><li><code>cache_size=1024.0</code>: Size of the cache during training in MiB.</li><li><code>max_iter=-1</code>: Limit the number of outer iterations in the solver. If <code>-1</code> (default) then <code>max_iter=100*n_samples</code>.</li><li><code>nochange_steps=1000</code>: Stop training if a <code>1e-3*tol</code> difference isn&#39;t seen in <code>nochange_steps</code> steps.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = SVR()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.LinearSVR" href="#RAPIDS.LinearSVR"><code>RAPIDS.LinearSVR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearSVR</code></pre><p>A model type for constructing a linear svr, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">LinearSVR = @load LinearSVR pkg=cuML Regression Methods</code></pre><p>Do <code>model = LinearSVR()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>LinearSVR(penalty=...)</code>.</p><p><code>SVR</code> is a wrapper for the RAPIDS Linear SVM Regressor.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>penalty=&quot;l2</code>: <code>l1</code> (Lasso) or <code>l2</code> (Ridge) penalty.</li><li><code>loss=&quot;epsilon_insensitive&quot;</code>: The loss term of the target function.</li><li><code>fit_intercept=true</code>: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.</li><li><code>penalized_intercept=true</code>: When true, the bias term is treated the same way as other features.</li><li><code>max_iter=1000</code>: Maximum number of iterations for the underlying solver.</li><li><code>linesearch_max_iter=1000</code>: Maximum number of linesearch (inner loop) iterations for the underlying (QN) solver.</li><li><code>lbfgs_memory=5</code>: Number of vectors approximating the hessian for the underlying QN solver (l-bfgs).</li><li><code>C=1.0</code>: The constant scaling factor of the loss term in the target formula <code>F(X, y) = penalty(X) + C * loss(X, y)</code>.</li><li><code>grad_tol=0.0001</code>: The threshold on the gradient for the underlying QN solver.</li><li><code>change_tol=0.00001</code>: The threshold on the function change for the underlying QN solver.</li><li><code>tol=nothing</code>: Tolerance for stopping criterion.</li><li><code>epsilon=0.0</code>: The epsilon-sensitivity parameter for the SVR loss function.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = LinearSVR()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.KNeighborsRegressor" href="#RAPIDS.KNeighborsRegressor"><code>RAPIDS.KNeighborsRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KNeighborsRegressor</code></pre><p>A model type for constructing a k neighbors regressor, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Regression Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">KNeighborsRegressor = @load KNeighborsRegressor pkg=cuML Regression Methods</code></pre><p>Do <code>model = KNeighborsRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>KNeighborsRegressor(n_neighbors=...)</code>.</p><p><code>KNeighborsRegressor</code> is a wrapper for the RAPIDS K-Nearest Neighbors Regressor.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li><li><code>y</code>: is an <code>AbstractVector</code> continuous target.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_neighbors=5</code>: Default number of neighbors to query.</li><li><code>algorithm=&quot;auto&quot;</code>: The query algorithm to use. </li><li><code>metric=&quot;euclidean&quot;</code>: Distance metric to use.</li><li><code>weights=&quot;uniform&quot;</code>: Sample weights to use. Currently, only the uniform strategy is supported.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given   features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions   are class assignments. </li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features</code>: the names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = rand(100)

model = KNeighborsRegressor()
mach = machine(model, X, y)
fit!(mach)
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><h2 id="Dimensionality-Reduction"><a class="docs-heading-anchor" href="#Dimensionality-Reduction">Dimensionality Reduction</a><a id="Dimensionality-Reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Dimensionality-Reduction" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.PCA" href="#RAPIDS.PCA"><code>RAPIDS.PCA</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PCA</code></pre><p>A model type for constructing a pca, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Dimensionality Reduction and Manifold Learning Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">PCA = @load PCA pkg=cuML Dimensionality Reduction and Manifold Learning Methods</code></pre><p>Do <code>model = PCA()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>PCA(copy=...)</code>.</p><p><code>PCA</code> is a wrapper for the RAPIDS PCA.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>copy=false</code>: If True, then copies data then removes mean from data.</li><li><code>iterated_power=15</code>: Used in Jacobi solver. The more iterations, the more accurate, but slower.</li><li><code>n_components=nothing</code>: The number of top K singular vectors / values you want.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>svd_solver=&quot;full</code>: <ul><li><code>full</code>: eigendecomposition of the covariance matrix then discards components.</li><li><code>jacobi</code>: much faster as it iteratively corrects, but is less accurate.</li></ul></li><li><code>tol=1e-7</code>: Convergence tolerance for <code>jacobi</code>. </li><li><code>whiten=false</code>: If True, de-correlates the components.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>tansform(mach, Xnew)</code></p></li><li><p><code>inverse_transform(mach, Xtrans)</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = PCA(n_components=2)
mach = machine(model, X)
fit!(mach)
X_trans = transform(mach, X)
inverse_transform(mach, X)

println(mach.fitresult.components_)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.IncrementalPCA" href="#RAPIDS.IncrementalPCA"><code>RAPIDS.IncrementalPCA</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IncrementalPCA</code></pre><p>A model type for constructing a incremental pca, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Dimensionality Reduction and Manifold Learning Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">IncrementalPCA = @load IncrementalPCA pkg=cuML Dimensionality Reduction and Manifold Learning Methods</code></pre><p>Do <code>model = IncrementalPCA()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>IncrementalPCA(copy=...)</code>.</p><p><code>IncrementalPCA</code> is a wrapper for the RAPIDS IncrementalPCA.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>copy=false</code>: If True, then copies data then removes mean from data.</li><li><code>n_components=nothing</code>: The number of top K singular vectors / values you want.</li><li><code>tol=1e-7</code>: Convergence tolerance for <code>jacobi</code>. </li><li><code>whiten=false</code>: If True, de-correlates the components.</li><li><code>batch_size=nothing</code>: The number of samples to use for each batch. Only used when calling <code>fit</code>.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>tansform(mach, Xnew)</code></p></li><li><p><code>inverse_transform(mach, Xtrans)</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = IncrementalPCA(n_components=2)
mach = machine(model, X)
fit!(mach)
X_trans = transform(mach, X)
inverse_transform(mach, X)

println(mach.fitresult.components_)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.TruncatedSVD" href="#RAPIDS.TruncatedSVD"><code>RAPIDS.TruncatedSVD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TruncatedSVD</code></pre><p>A model type for constructing a truncated svd, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Dimensionality Reduction and Manifold Learning Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">TruncatedSVD = @load TruncatedSVD pkg=cuML Dimensionality Reduction and Manifold Learning Methods</code></pre><p>Do <code>model = TruncatedSVD()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>TruncatedSVD(n_components=...)</code>.</p><p><code>TruncatedSVD</code> is a wrapper for the RAPIDS TruncatedSVD.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>n_components=nothing</code>: The number of top K singular vectors / values you want.</li><li><code>n_iter=15</code>: The number of top K singular vectors / values you want.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>tol=1e-7</code>: Convergence tolerance for <code>jacobi</code>.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>tansform(mach, Xnew)</code></p></li><li><p><code>inverse_transform(mach, Xtrans)</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = TruncatedSVD(n_components=2)
mach = machine(model, X)
fit!(mach)
X_trans = transform(mach, X)
inverse_transform(mach, X_trans)

println(mach.fitresult.components_)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.UMAP" href="#RAPIDS.UMAP"><code>RAPIDS.UMAP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">UMAP</code></pre><p>A model type for constructing a umap, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Dimensionality Reduction and Manifold Learning Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UMAP = @load UMAP pkg=cuML Dimensionality Reduction and Manifold Learning Methods</code></pre><p>Do <code>model = UMAP()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UMAP(n_neighbors=...)</code>.</p><p><code>UMAP</code> is a wrapper for the RAPIDS UMAP.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>n_neighbors=15</code>: The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.</li><li><code>n_components=2</code>: The dimension of the space to embed into.</li><li><code>metric=&quot;euclidean&quot;</code>: <code>l1</code>, <code>cityblock</code>, <code>taxicab</code>, <code>manhattan</code>, <code>euclidean</code>, <code>l2</code>, <code>sqeuclidean</code>, <code>canberra</code>, <code>minkowski</code>, <code>chebyshev</code>, <code>linf</code>, <code>cosine</code>, <code>correlation</code>, <code>hellinger</code>, <code>hamming</code>, <code>jaccard</code></li><li><code>n_epochs=nothing</code>: The number of training epochs to be used in optimizing the low dimensional embedding. </li><li><code>learning_rate=1.0</code>: The initial learning rate for the embedding optimization.</li><li><code>init=&quot;spectral&quot;</code>: How to initialize the low dimensional embedding. <ul><li><code>spectral</code>: use a spectral embedding of the fuzzy 1-skeleton.</li><li><code>random</code>: assign initial embedding positions at random.</li></ul></li><li><code>min_dist=0.1</code>: The effective minimum distance between embedded points.</li><li><code>spread=1.0</code>: The effective scale of embedded points.</li><li><code>set_op_mix_ratio=1.0</code>: Interpolate between (fuzzy) union and intersection as the set operation used to combine local fuzzy simplicial sets to obtain a global fuzzy simplicial sets.</li><li><code>local_connectivity=1</code>: The local connectivity required - i.e. the number of nearest neighbors that should be assumed to be connected at a local level. </li><li><code>repulsion_strength=1.0</code>: Weighting applied to negative samples in low dimensional embedding optimization.</li><li><code>negative_sample_rate=5</code>: The number of negative samples to select per positive sample in the optimization process.</li><li><code>transform_queue_size=4.0</code>: For transform operations (embedding new points using a trained model this will control how aggressively to search for nearest neighbors.</li><li><code>a=nothing</code>: More specific parameters controlling the embedding.</li><li><code>b=nothing</code>: More specific parameters controlling the embedding.</li><li><code>hash_input=false</code>: Hash input, so exact embeddings are return when transform is called on the same data upon which the model was trained.</li><li><code>random_state=nothing</code>: Seed for the random number generator.`</li><li><code>callback=nothing</code>: Used to intercept the internal state of embeddings while they are being trained.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>tansform(mach, Xnew)</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = UMAP(n_components=2)
mach = machine(model, X)
fit!(mach)
X_trans = transform(mach, X)

println(mach.fitresult.embedding_)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.TSNE" href="#RAPIDS.TSNE"><code>RAPIDS.TSNE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TSNE</code></pre><p>A model type for constructing a tsne, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Dimensionality Reduction and Manifold Learning Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">TSNE = @load TSNE pkg=cuML Dimensionality Reduction and Manifold Learning Methods</code></pre><p>Do <code>model = TSNE()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>TSNE(n_components=...)</code>.</p><p><code>TSNE</code> is a wrapper for the RAPIDS TSNE.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>n_components=2</code>: The output dimensionality size. Currently only 2 is supported.</li><li><code>perplexity=30.0</code></li><li><code>early_exaggeration=12.0</code>: Space between clusters.</li><li><code>late_exaggeration=1.0</code>: Space between clusters. It may be beneficial to increase this slightly to improve cluster separation.</li><li><code>learning_rate=200.0</code>: The learning rate usually between (10, 1000).</li><li><code>n_iter</code>: Number of epochs. The more epochs, the more stable/accurate the final embedding.</li><li><code>n_iter_without_progress=300</code>: Currently unused. When the KL Divergence becomes too small after some iterations, terminate t-SNE early.</li><li><code>min_grad_norm=1e-7</code>: The minimum gradient norm for when t-SNE will terminate early. Used in the <code>exact</code> and <code>fft</code> algorithms.</li><li><code>metric=&quot;euclidean&quot;</code>: <code>l1</code>, <code>cityblock</code>, <code>manhattan</code>, <code>euclidean</code>, <code>l2</code>, <code>sqeuclidean</code>, <code>minkowski</code>, <code>chebyshev</code>, <code>cosine</code>, <code>correlation</code></li><li><code>init=&quot;random&quot;</code>: Only <code>random</code> is supported.</li><li><code>method=&quot;fft&quot;</code>: <code>barnes_hut</code> and <code>fft</code> are fast approximations. <code>exact</code> is more accurate but slower.</li><li><code>angle=0.5</code>: Valid values are between 0.0 and 1.0, which trade off speed and accuracy, respectively.</li><li><code>learning_rate_method=&quot;adaptive&quot;</code>: <code>adaptive</code> or <code>none</code>.</li><li><code>n_neighbors=90</code>: The number of datapoints you want to use in the attractive forces.</li><li><code>perplexity_max_iter=100</code>: The number of epochs the best gaussian bands are found for.</li><li><code>exaggeration_iter=250</code>: To promote the growth of clusters, set this higher.</li><li><code>pre_momentum=0.5</code>: During the exaggeration iteration, more forcefully apply gradients.</li><li><code>post_momentum=0.8</code>: During the late phases, less forcefully apply gradients.</li><li><code>square_distances=true</code>: Whether TSNE should square the distance values.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>tansform(mach, Xnew)</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = TSNE(n_components=2)
mach = machine(model, X)
fit!(mach)
X_trans = transform(mach, X)

println(mach.fitresult.kl_divergence_)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.GaussianRandomProjection" href="#RAPIDS.GaussianRandomProjection"><code>RAPIDS.GaussianRandomProjection</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianRandomProjection</code></pre><p>A model type for constructing a gaussian random projection, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Dimensionality Reduction and Manifold Learning Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">GaussianRandomProjection = @load GaussianRandomProjection pkg=cuML Dimensionality Reduction and Manifold Learning Methods</code></pre><p>Do <code>model = GaussianRandomProjection()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>GaussianRandomProjection(n_components=...)</code>.</p><p><code>GaussianRandomProjection</code> is a wrapper for the RAPIDS GaussianRandomProjection.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>n_components=&quot;auto&quot;</code>: Dimensionality of the target projection space. </li><li><code>eps=0.1</code>: Error tolerance during projection.</li><li><code>random_state=nothing</code>: Seed for the random number generator.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>tansform(mach, Xnew)</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)

model = GaussianRandomProjection(n_components=2)
mach = machine(model, X)
fit!(mach)
X_trans = transform(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><h2 id="Time-Series"><a class="docs-heading-anchor" href="#Time-Series">Time Series</a><a id="Time-Series-1"></a><a class="docs-heading-anchor-permalink" href="#Time-Series" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.ExponentialSmoothing" href="#RAPIDS.ExponentialSmoothing"><code>RAPIDS.ExponentialSmoothing</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExponentialSmoothing</code></pre><p>A model type for constructing a exponential smoothing, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Time Series Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">ExponentialSmoothing = @load ExponentialSmoothing pkg=cuML Time Series Methods</code></pre><p>Do <code>model = ExponentialSmoothing()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ExponentialSmoothing(seasonal=...)</code>.</p><p><code>ExponentialSmoothing</code> is a wrapper for the RAPIDS HoltWinters time series analysis model.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>seasonal=&quot;additive&quot;</code>: Whether the seasonal trend should be calculated additively or multiplicatively.</li><li><code>seasonal_periods=2</code>: The seasonality of the data (how often it repeats). For monthly data this should be 12, for weekly data, this should be 7.</li><li><code>start_periods=2</code>: Number of seasons to be used for seasonal seed values.</li><li><code>eps=2.24e-3</code>: The accuracy to which gradient descent should achieve.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>forecast(mach, n_timesteps)</code>: return a forecasted series of <code>n_timesteps</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = ExponentialSmoothing()
mach = machine(model, X)
fit!(mach)
preds = forecast(mach, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RAPIDS.ARIMA" href="#RAPIDS.ARIMA"><code>RAPIDS.ARIMA</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ARIMA</code></pre><p>A model type for constructing a arima, based on <a href="https://github.com/tylerjthomas9/RAPIDS.jl">cuML Time Series Methods.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">ARIMA = @load ARIMA pkg=cuML Time Series Methods</code></pre><p>Do <code>model = ARIMA()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ARIMA(exog=...)</code>.</p><p><code>ARIMA</code> is a wrapper for the RAPIDS batched ARIMA model for in- and out-of-sample time-series prediction, with support for seasonality (SARIMA).</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y)</p><p>where</p><ul><li><code>X</code>: any table or array of input features (eg, a <code>DataFrame</code>) whose columns   each have one of the following element scitypes: <code>Continuous</code></li></ul><p><strong>Hyper-parameters</strong></p><ul><li><code>order=(1,1,1)</code>: The ARIMA order (p, d, q) of the model.</li><li><code>seasonal_order=(0,0,0,0)</code>:The seasonal ARIMA order (P, D, Q, s) of the model.</li><li><code>exog=nothing</code>: Exogenous variables, assumed to have each time series in columns, such that variables associated with a same batch member are adjacent. <ul><li>This must be a PyArray</li></ul></li><li><code>verbose=false</code>: Sets logging level.</li><li><code>fit_intercept=true</code>: If True, include a constant trend mu in the model.</li><li><code>simple_differencing=true</code>: If True, the data is differenced before being passed to the Kalman filter.                            -If False, differencing is part of the state-space model.</li><li><code>verbose=false</code>: Sets logging level.</li></ul><p><strong>Operations</strong></p><ul><li><code>forecast(mach, n_timesteps)</code>: return a forecasted series of <code>n_timesteps</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>model</code>: the trained model object created by the RAPIDS.jl package</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using RAPIDS
using MLJBase

X = rand(100, 5)
y = [repeat([0], 50)..., repeat([1], 50)...]

model = ARIMA()
mach = machine(model, X)
fit!(mach)
preds = forecast(mach, 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/tylerjthomas9/RAPIDS.jl.git">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../python_api/">« Python API</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 16 November 2022 22:17">Wednesday 16 November 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
