var documenterSearchIndex = {"docs":
[{"location":"python_api/#Python-API","page":"Python API","title":"Python API","text":"","category":"section"},{"location":"python_api/","page":"Python API","title":"Python API","text":"You can directly interface with the Python API for all RAPIDS packages. By default, the following packages are exported:","category":"page"},{"location":"python_api/","page":"Python API","title":"Python API","text":"cupy\ncudf\ncuml\ncugraph\ncusignal\ncuspatial\ncuxfilter\ndask\ndask_cuda\ndask_cudf\nnumpy\npickle","category":"page"},{"location":"python_api/#CUML-Example-Classification","page":"Python API","title":"CUML Example - Classification","text":"","category":"section"},{"location":"python_api/","page":"Python API","title":"Python API","text":"using RAPIDS\nusing PythonCall\n\nX_numpy = numpy.random.rand(100, 5)\ny_numpy = numpy.random.randint(0, 2, 100)\n\nmodel = cuml.LogisticRegression()\nmodel.fit(X_numpy, y_numpy)\npreds_numpy = model.predict(X_numpy)\npreds = pyconvert(Array, preds_numpy)\n\nprintln(model.coef_)","category":"page"},{"location":"python_api/#CUML-Example-Regression","page":"Python API","title":"CUML Example - Regression","text":"","category":"section"},{"location":"python_api/","page":"Python API","title":"Python API","text":"using RAPIDS\nusing PythonCall\n\nX_numpy = numpy.random.rand(100, 5)\ny_numpy = numpy.random.rand(100)\n\nmodel = cuml.LinearRegression()\nmodel.fit(X_numpy, y_numpy)\npreds_numpy = model.predict(X_numpy)\npreds = pyconvert(Array, preds_numpy)\n\nprintln(model.coef_)","category":"page"},{"location":"#RAPIDS.jl-Docs","page":"Home","title":"RAPIDS.jl Docs","text":"","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"From source:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add https://github.com/tylerjthomas9/RAPIDS.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(url=\"https://github.com/tylerjthomas9/RAPIDS.jl\")","category":"page"},{"location":"cuml/#MLJ-API","page":"cuMl","title":"MLJ API","text":"","category":"section"},{"location":"cuml/#MLJ-Example-Classification","page":"cuMl","title":"MLJ Example - Classification","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"using RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = LogisticRegression()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\nprintln(mach.fitresult.coef_)","category":"page"},{"location":"cuml/#MLJ-Example-Regression","page":"cuMl","title":"MLJ Example - Regression","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"using RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = LinearRegression()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\nprintln(mach.fitresult.coef_)","category":"page"},{"location":"cuml/#Clustering","page":"cuMl","title":"Clustering","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"KMeans\nDBSCAN\nAgglomerativeClustering\nHDBSCAN","category":"page"},{"location":"cuml/#RAPIDS.KMeans","page":"cuMl","title":"RAPIDS.KMeans","text":"KMeans\n\nA model type for constructing a k means, based on cuML Clustering Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKMeans = @load KMeans pkg=cuML Clustering Methods\n\nDo model = KMeans() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KMeans(n_clusters=...).\n\nKMeans is a wrapper for the RAPIDS KMeans Clustering.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nn_clusters=8: The number of clusters/centroids.\nmax_iter=300: Maximum iterations of the EM algorithm. \ntol=1e-4: Stopping criterion when centroid means do not change much.\nrandom_state=1: Seed for the random number generator.\ninit=\"scalable-k-means++\"\nscalable-k-means++ or k-means||: Uses fast and stable scalable kmeans++ initialization.\nrandom: Choose n_cluster observations (rows) at random from data for the initial centroids.\nn_init=1: Number of instances the k-means algorithm will be called with different seeds. The final results will be from the instance that produces lowest inertia out of n_init instances.\noversampling_factor=20: The amount of points to sample in scalable k-means++ initialization for potential centroids.\nmax_samples_per_batch=32768: The number of data samples to use for batches of the pairwise distance computation.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\nlabels: Vector of observation labels. \n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = KMeans()\nmach = machine(model, X)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.DBSCAN","page":"cuMl","title":"RAPIDS.DBSCAN","text":"DBSCAN\n\nA model type for constructing a dbscan, based on cuML Clustering Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDBSCAN = @load DBSCAN pkg=cuML Clustering Methods\n\nDo model = DBSCAN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DBSCAN(eps=...).\n\nDBSCAN is a wrapper for the RAPIDS DBSCAN Clustering.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\neps=0.5: The maximum distance between 2 points such they reside in the same neighborhood.\nmin_samples=5: The number of samples in a neighborhood such that this group can be considered as an important core point (including the point itself).\nmetric=\"euclidean: The metric to use when calculating distances between points.\neuclidean, cosine, precomputed\nmax_mbytes_per_batch=nothing: Calculate batch size using no more than this number of megabytes for the pairwise distance computation. \nverbose=false: Sets logging level.\n\nOperations\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\nlabels: Vector of observation labels. \n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = DBSCAN()\nmach = machine(model, X)\nfit!(mach)\npreds = mach.report.labels #DBSCAN does not have a predict method\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.AgglomerativeClustering","page":"cuMl","title":"RAPIDS.AgglomerativeClustering","text":"AgglomerativeClustering\n\nA model type for constructing a agglomerative clustering, based on cuML Clustering Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAgglomerativeClustering = @load AgglomerativeClustering pkg=cuML Clustering Methods\n\nDo model = AgglomerativeClustering() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AgglomerativeClustering(verbose=...).\n\nAgglomerativeClustering is a wrapper for the RAPIDS Agglomerative Clustering.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nn_clusters=8: The number of clusters.\naffinity=\"euclidean\": Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, or “cosine”. \nn_neighbors=15: The number of neighbors to compute when connectivity = “knn”\nconnectivity=\"knn\":\nknn will sparsify the fully-connected connectivity matrix to save memory and enable much larger inputs.\npairwise will compute the entire fully-connected graph of pairwise distances between each set of points.\nverbose=false: Sets logging level.\n\nOperations\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\nlabels: Vector of observation labels. \n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = AgglomerativeClustering()\nmach = machine(model, X)\nfit!(mach)\npreds = mach.report.labels #AgglomerativeClustering does not have a predict method\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.HDBSCAN","page":"cuMl","title":"RAPIDS.HDBSCAN","text":"HDBSCAN\n\nA model type for constructing a hdbscan, based on cuML Clustering Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHDBSCAN = @load HDBSCAN pkg=cuML Clustering Methods\n\nDo model = HDBSCAN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in HDBSCAN(alpha=...).\n\nHDBSCAN is a wrapper for the RAPIDS HDBSCAN Clustering.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nalpha=1.0: A distance scaling parameter as used in robust single linkage.\nmin_cluster_size=5: The minimum number of samples in a group for that group to be considered a cluster.\nmin_samples=nothing: The number of samples in a neighborhood for a point to be considered as a core point.\ncluster_selection_epsilon=0.0: A distance threshold. Clusters below this value will be merged.\nmax_cluster_size=0: A limit to the size of clusters returned by the eom algorithm.\np=2: p value to use if using the minkowski metric.\ncluster_selection_method=\"eom\": The method used to select clusters from the condensed tree. eom/leaf\nallow_single_cluster=false: Allow HDBSCAN to produce a single cluster.\nverbose=false: Sets logging level.\n\nOperations\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\nlabels: Vector of observation labels. \n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = HDBSCAN()\nmach = machine(model, X)\nfit!(mach)\npreds = mach.report.labels #HDBSCAN does not have a predict method\n\n\n\n\n\n","category":"type"},{"location":"cuml/#Classification","page":"cuMl","title":"Classification","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"LogisticRegression\nMBSGDClassifier\nRandomForestClassifier\nSVC\nLinearSVC\nKNeighborsClassifier","category":"page"},{"location":"cuml/#RAPIDS.LogisticRegression","page":"cuMl","title":"RAPIDS.LogisticRegression","text":"LogisticRegression\n\nA model type for constructing a logistic regression, based on cuML Classification Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLogisticRegression = @load LogisticRegression pkg=cuML Classification Methods\n\nDo model = LogisticRegression() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LogisticRegression(penalty=...).\n\nLogisticRegression is a wrapper for the RAPIDS Logistic Regression.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is the target, which can be any AbstractVector whose element   scitype is <:OrderedFactor or <:Multiclass; check the scitype   with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\npenalty=\"l2\": Normalization/penalty function (\"none\", \"l1\", \"l2\", \"elasticnet\").\nnone: the L-BFGS solver will be used\nl1: The L1 penalty is best when there are only a few useful features (sparse), and you       want to zero out non-important features. The L-BFGS solver will be used.\nl2: The L2 penalty is best when you have a lot of important features, especially if they       are correlated.The L-BFGS solver will be used.\nelasticnet: A combination of the L1 and L2 penalties. The OWL-QN solver will be used if               l1_ratio>0, otherwise the L-BFGS solver will be used.\n`tol=1e-4': Tolerance for stopping criteria. \nC=1.0: Inverse of regularization strength.\nfit_intercept=true: If True, the model tries to correct for the global mean of y.                        If False, the model expects that you have centered the data.\nclass_weight=\"balanced\": Dictionary or \"balanced\".\nmax_iter=1000: Maximum number of iterations taken for the solvers to converge.\nlinesearch_max_iter=50: Max number of linesearch iterations per outer iteration used in                            the lbfgs and owl QN solvers.\nsolver=\"qn\": Algorithm to use in the optimization problem. Currently only qn is                supported, which automatically selects either L-BFGSor OWL-QN\nl1_ratio=nothing: The Elastic-Net mixing parameter. \nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \npredict_proba(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are probabilistic, but uncalibrated.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = LogisticRegression()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.MBSGDClassifier","page":"cuMl","title":"RAPIDS.MBSGDClassifier","text":"MBSGDClassifier\n\nA model type for constructing a mbsgd classifier, based on cuML Classification Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMBSGDClassifier = @load MBSGDClassifier pkg=cuML Classification Methods\n\nDo model = MBSGDClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MBSGDClassifier(loss=...).\n\nMBSGDClassifier is a wrapper for the RAPIDS Mini Batch SGD Classifier.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector finite target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nloss=\"squared_loss\": Loss function (\"hinge\", \"log\", \"squared_loss\").\nhinge: Linear SVM\nlog: Logistic regression\nsquared_loss: Linear regression\npenalty=\"none\": Normalization/penalty function (\"none\", \"l1\", \"l2\", \"elasticnet\").\nnone: the L-BFGS solver will be used\nl1: The L1 penalty is best when there are only a few useful features (sparse), and you       want to zero out non-important features. The L-BFGS solver will be used.\nl2: The L2 penalty is best when you have a lot of important features, especially if they       are correlated.The L-BFGS solver will be used.\nelasticnet: A combination of the L1 and L2 penalties. The OWL-QN solver will be used if               l1_ratio>0, otherwise the L-BFGS solver will be used.\nalpha=1e-4: The constant value which decides the degree of regularization.\nl1_ratio=nothing: The Elastic-Net mixing parameter. \nbatch_size: The number of samples in each batch.\nfit_intercept=true: If True, the model tries to correct for the global mean of y.                        If False, the model expects that you have centered the data.\nepochs=1000: The number of times the model should iterate through the entire dataset during training.\n`tol=1e-3': The training process will stop if currentloss > previousloss - tol.\nshuffle=true: If true, shuffles the training data after each epoch.\neta0=1e-3: The initial learning rate.\npower_t=0.5: The exponent used for calculating the invscaling learning rate.\nlearning_rate=\"constant: Method for modifying the learning rate during training                           (\"adaptive\", \"constant\", \"invscaling\", \"optimal\")\noptimal: not supported\nconstant: constant learning rate\nadaptive: changes the learning rate if the training loss or the validation accuracy does               not improve for niterno_change epochs. The old learning rate is generally divided by 5.\ninvscaling: eta = eta0 / pow(t, power_t)\nn_iter_no_change=5: the number of epochs to train without any imporvement in the model\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \npredict_proba(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are probabilistic, but uncalibrated.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = MBSGDClassifier()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.RandomForestClassifier","page":"cuMl","title":"RAPIDS.RandomForestClassifier","text":"RandomForestClassifier\n\nA model type for constructing a random forest classifier, based on cuML Classification Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomForestClassifier = @load RandomForestClassifier pkg=cuML Classification Methods\n\nDo model = RandomForestClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomForestClassifier(n_estimators=...).\n\nRandomForestClassifier is a wrapper for the RAPIDS RandomForestClassifier.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector finite target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_estimators=100: The total number of trees in the forest.\nsplit_creation=2: The criterion used to split nodes\n0 or gini for gini impurity\n1 or entropy for information gain (entropy)\nbootstrap=true: If true, each tree in the forest is built using a bootstrap sample with replacement.\nmax_samples=1.0: Ratio of dataset rows used while fitting each tree.\nmax_depth=16: Maximum tree depth.\nmax_leaves=-1: Maximum leaf nodes per tree. Soft constraint. Unlimited, If -1.\nmax_features=\"auto\": Ratio of number of features (columns) to consider per node split.\nIf type Int then max_features is the absolute count of features to be used.\nIf type Float64 then max_features is a fraction.\nIf auto then max_features=n_features = 1.0.\nIf sqrt then max_features=1/sqrt(n_features).\nIf log2 then max_features=log2(n_features)/n_features.\nIf None, then max_features=1.0.\nn_bins=128: Maximum number of bins used by the split algorithm per feature.\nn_streams=4: Number of parallel streams used for forest building\nmin_samples_leaf=1: The minimum number of samples in each leaf node.\nIf type Int, then min_samples_leaf represents the minimum number.\nIf Float64, then min_samples_leaf represents a fraction and ceil(min_samples_leaf * n_rows)   is the minimum number of samples for each leaf node.\nmin_samples_split=2: The minimum number of samples required to split an internal node.\nIf type Int, then min_samples_split represents the minimum number.\nIf Float64, then min_samples_split represents a fraction and ceil(min_samples_leaf * n_rows)   is the minimum number of samples for each leaf node.\nmin_impurity_decrease=0.0: The minimum decrease in impurity required for node to be split.\nmax_batch_size=4096: Maximum number of nodes that can be processed in a given batch.\nrandom_state=nothing: Seed for the random number generator.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = RandomForestClassifier()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.SVC","page":"cuMl","title":"RAPIDS.SVC","text":"SVC\n\nA model type for constructing a svc, based on cuML Classification Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVC = @load SVC pkg=cuML Classification Methods\n\nDo model = SVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVC(C=...).\n\nSVC is a wrapper for the RAPIDS SVC.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is the target, which can be any AbstractVector whose element   scitype is <:OrderedFactor or <:Multiclass; check the scitype   with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nC=1.0: The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\nkernel=\"rbf\": linear, poly, rbf, sigmoid are supported.\ndegree=3: Degree of polynomial kernel function.\ngamma=\"scale\"\nauto: gamma will be set to 1 / n_features\nscale: gamma will be set to 1 / (n_features * var(X))\ncoef0=0.0: Independent term in kernel function, only signifficant for poly and sigmoid.\ntol=0.001: Tolerance for stopping criterion.\ncache_size=1024.0: Size of the cache during training in MiB.\nclass_weight=nothing: Weights to modify the parameter C for class i to class_weight[i]*C. The string \"balanced\"` is also accepted.\nmax_iter=-1: Limit the number of outer iterations in the solver. If -1 (default) then max_iter=100*n_samples.\nmulticlass_strategy=\"ovo\"\novo: OneVsOneClassifier\novr: OneVsRestClassifier\nnochange_steps=1000: Stop training if a 1e-3*tol difference isn't seen in nochange_steps steps.\nprobability=false: Enable or disable probability estimates.\nrandom_state=nothing: Seed for the random number generator.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \npredict_proba(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are probabilistic, but uncalibrated.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = SVC()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.LinearSVC","page":"cuMl","title":"RAPIDS.LinearSVC","text":"LinearSVC\n\nA model type for constructing a linear svc, based on cuML Classification Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearSVC = @load LinearSVC pkg=cuML Classification Methods\n\nDo model = LinearSVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearSVC(penalty=...).\n\nLinearSVC is a wrapper for the RAPIDS LinearSVC.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is the target, which can be any AbstractVector whose element   scitype is <:OrderedFactor or <:Multiclass; check the scitype   with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\npenalty=\"l2: l1 (Lasso) or l2 (Ridge) penalty.\nloss=\"squared_hinge\": The loss term of the target function.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\npenalized_intercept=true: When true, the bias term is treated the same way as other features.\nmax_iter=1000: Maximum number of iterations for the underlying solver.\nlinesearch_max_iter=1000: Maximum number of linesearch (inner loop) iterations for the underlying (QN) solver.\nlbfgs_memory=5: Number of vectors approximating the hessian for the underlying QN solver (l-bfgs).\nC=1.0: The constant scaling factor of the loss term in the target formula F(X, y) = penalty(X) + C * loss(X, y).\ngrad_tol=0.0001: The threshold on the gradient for the underlying QN solver.\nchange_tol=0.00001: The threshold on the function change for the underlying QN solver.\ntol=nothing: Tolerance for stopping criterion.\nprobabability=false: Enable or disable probability estimates.\nmulti_class=\"ovo\"\novo: OneVsOneClassifier\novr: OneVsRestClassifier\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \npredict_proba(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are probabilistic, but uncalibrated.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = LinearSVC()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.KNeighborsClassifier","page":"cuMl","title":"RAPIDS.KNeighborsClassifier","text":"KNeighborsClassifier\n\nA model type for constructing a k neighbors classifier, based on cuML Classification Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKNeighborsClassifier = @load KNeighborsClassifier pkg=cuML Classification Methods\n\nDo model = KNeighborsClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KNeighborsClassifier(algorithm=...).\n\nKNeighborsClassifier is a wrapper for the RAPIDS K-Nearest Neighbors Classifier.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is the target, which can be any AbstractVector whose element   scitype is <:OrderedFactor or <:Multiclass; check the scitype   with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_neighbors=5: Default number of neighbors to query.\nalgorithm=\"brute\": Only one algorithm is currently supported.\nmetric=\"euclidean\": Distance metric to use.\nweights=\"uniform\": Sample weights to use. Currently, only the uniform strategy is supported.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \npredict_proba(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are probabilistic, but uncalibrated.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = KNeighborsClassifier()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#Regression","page":"cuMl","title":"Regression","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"LinearRegression\nRidge\nLasso\nElasticNet\nMBSGDRegressor\nRandomForestRegressor\nCD\nSVR\nLinearSVR\nKNeighborsRegressor","category":"page"},{"location":"cuml/#RAPIDS.LinearRegression","page":"cuMl","title":"RAPIDS.LinearRegression","text":"LinearRegression\n\nA model type for constructing a linear regression, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearRegression = @load LinearRegression pkg=cuML Regression Methods\n\nDo model = LinearRegression() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearRegression(algorithm=...).\n\nLinearRegression is a wrapper for the RAPIDS Linear Regression.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nalgorithm=\"eig\": \neig: use an eigendecomposition of the covariance matrix.\nqr: use QR decomposition algorithm and solve Rx = Q^T y\nsvd: alias for svd-jacobi.\nsvd-qr: compute SVD decomposition using QR algorithm.\nsvd-jacobi: compute SVD decomposition using Jacobi iterations.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nnormalize=true: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation.                    If false, no scaling will be done. \nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = LinearRegression()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.Ridge","page":"cuMl","title":"RAPIDS.Ridge","text":"Ridge\n\nA model type for constructing a ridge, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidge = @load Ridge pkg=cuML Regression Methods\n\nDo model = Ridge() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Ridge(alpha=...).\n\nRidge is a wrapper for the RAPIDS Ridge Regression.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nalpha=1.0: Regularization strength - must be a positive float. Larger values specify stronger regularization.\nsolver=\"eig\": \ncd: use coordinate descent. Very fast and is suitable for large problems.\neig: use an eigendecomposition of the covariance matrix.\nsvd: alias for svd-jacobi. Slower, but guaranteed to be stable.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nnormalize=true: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation.                    If false, no scaling will be done. \nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = Ridge()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.Lasso","page":"cuMl","title":"RAPIDS.Lasso","text":"Lasso\n\nA model type for constructing a lasso, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLasso = @load Lasso pkg=cuML Regression Methods\n\nDo model = Lasso() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Lasso(alpha=...).\n\nLasso is a wrapper for the RAPIDS Lasso Regression.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nalpha=1.0: Constant that multiplies the L1 term. alpha = 0 is equivalent to an ordinary least square.\ntol=1e-4: Tolerance for stopping criteria. \nmax_iter=1000: Maximum number of iterations taken for the solvers to converge.\nsolver=\"cd\": \ncd: Coordinate descent.\nqn: quasi-newton. You may find the alternative ‘qn’ algorithm is faster when the number of features is sufficiently large, but the sample size is small.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nnormalize=true: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation.                    If false, no scaling will be done. \nselection=\"cyclic\": \ncyclic: loop over features to update coefficients.\nrandom: a random coefficient is updated every iteration.\na random coefficient is updated every iteration rather than looping over features sequentially by default.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = Lasso()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.ElasticNet","page":"cuMl","title":"RAPIDS.ElasticNet","text":"ElasticNet\n\nA model type for constructing a elastic net, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nElasticNet = @load ElasticNet pkg=cuML Regression Methods\n\nDo model = ElasticNet() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ElasticNet(alpha=...).\n\nElasticNet is a wrapper for the RAPIDS ElasticNet Regression.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nalpha=1.0: Constant that multiplies the L1 term. alpha = 0 is equivalent to an ordinary least square.\nl1_ratio=0.5: The ElasticNet mixing parameter.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nnormalize=true: This parameter is ignored when fit_intercept is set to false.                   If true, the predictors in X will be normalized by dividing by the column-wise standard deviation. \nmax_iter=1000: Maximum number of iterations taken for the solvers to converge.\ntol=1e-3: Tolerance for stopping criteria. \nsolver=\"cd\": \ncd: Coordinate descent.\nqn: quasi-newton. You may find the alternative ‘qn’ algorithm is faster when the number of features is sufficiently large, but the sample size is small.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nselection=\"cyclic\": \ncyclic: loop over features to update coefficients.\nrandom: a random coefficient is updated every iteration.\na random coefficient is updated every iteration rather than looping over features sequentially by default.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = ElasticNet()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.MBSGDRegressor","page":"cuMl","title":"RAPIDS.MBSGDRegressor","text":"MBSGDRegressor\n\nA model type for constructing a mbsgd regressor, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMBSGDRegressor = @load MBSGDRegressor pkg=cuML Regression Methods\n\nDo model = MBSGDRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MBSGDRegressor(loss=...).\n\nMBSGDRegressor is a wrapper for the RAPIDS MBSGDRegressor.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nloss=\"squared_loss: squared_loss uses linear regression\npenalty=\"none: \nnone: No regularization.\nl1: L1 norm (Lasso).\nl2: L2 norm (Ridge).\nelasticnet: weighted average of L1 and L2 norms.\nalpha=0.0001: The constant value which decides the degree of regularization.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nl1_ratio=0.5: The ElasticNet mixing parameter.\nbatch_size=32: The number of samples in each batch.\nepochs=1000: The number of times the model should iterate through the entire dataset.\ntol=1e-3: Tolerance for stopping criteria.\nshuffle=true: True, shuffles the training data after each epoch False, does not shuffle the training data after each epoch\neta_0=0.001: Initial learning rate.\npower_t=0.5: The exponent used for calculating the invscaling learning rate.\nlearning_rate=\"constant:\nn_iter_no_change=5: The number of epochs to train without any improvement in the model.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = MBSGDRegressor()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.RandomForestRegressor","page":"cuMl","title":"RAPIDS.RandomForestRegressor","text":"RandomForestRegressor\n\nA model type for constructing a random forest regressor, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomForestRegressor = @load RandomForestRegressor pkg=cuML Regression Methods\n\nDo model = RandomForestRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomForestRegressor(n_estimators=...).\n\nRandomForestRegressor is a wrapper for the RAPIDS RandomForestRegressor.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_estimators=100: The total number of trees in the forest.\nsplit_creation=2: The criterion used to split nodes\n2 or mse for mean squared error\n4 or poisson for poisson half deviance\n5 or gamma for gamma half deviance\n6 or inverse_gaussian for inverse gaussian deviance\nbootstrap=true: If true, each tree in the forest is built using a bootstrap sample with replacement.\nmax_samples=1.0: Ratio of dataset rows used while fitting each tree.\nmax_depth=16: Maximum tree depth.\nmax_leaves=-1: Maximum leaf nodes per tree. Soft constraint. Unlimited, If -1.\nmax_features=\"auto\": Ratio of number of features (columns) to consider per node split.\nIf type Int then max_features is the absolute count of features to be used.\nIf type Float64 then max_features is a fraction.\nIf auto then max_features=n_features = 1.0.\nIf sqrt then max_features=1/sqrt(n_features).\nIf log2 then max_features=log2(n_features)/n_features.\nIf None, then max_features=1.0.\nn_bins=128: Maximum number of bins used by the split algorithm per feature.\nn_streams=4: Number of parallel streams used for forest building\nmin_samples_leaf=1: The minimum number of samples in each leaf node.\nIf type Int, then min_samples_leaf represents the minimum number.\nIf Float64, then min_samples_leaf represents a fraction and ceil(min_samples_leaf * n_rows)   is the minimum number of samples for each leaf node.\nmin_samples_split=2: The minimum number of samples required to split an internal node.\nIf type Int, then min_samples_split represents the minimum number.\nIf Float64, then min_samples_split represents a fraction and ceil(min_samples_leaf * n_rows)   is the minimum number of samples for each leaf node.\nmin_impurity_decrease=0.0: The minimum decrease in impurity required for node to be split.\naccuracy_metric=\"r2\"\nr2: r-squared\nmedian_ae: median of absolute error\nmean_ae: mean of absolute error\nmse: mean squared error\nmax_batch_size=4096: Maximum number of nodes that can be processed in a given batch.\nrandom_state=nothing: Seed for the random number generator.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training., in an order consistent with the output of print_tree (see below)\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = RandomForestRegressor()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.CD","page":"cuMl","title":"RAPIDS.CD","text":"CD\n\nA model type for constructing a cd, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCD = @load CD pkg=cuML Regression Methods\n\nDo model = CD() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CD(loss=...).\n\nCD is a wrapper for the RAPIDS Coordinate Descent.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nalpha=0.0001: Regularization strength. alpha = 0 is equivalent to an ordinary least square.\nl1_ratio=0.15: The ElasticNet mixing parameter.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\nnormalize=false: If true, the data is normalized.\ntol=0.001: The tolerance for the optimization: if the updates are smaller than tol, solver stops.\nshuffle=true: If true, a random coefficient is updated at each iteration. \nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = CD()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.SVR","page":"cuMl","title":"RAPIDS.SVR","text":"SVR\n\nA model type for constructing a svr, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVR = @load SVR pkg=cuML Regression Methods\n\nDo model = SVR() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVR(C=...).\n\nSVR is a wrapper for the RAPIDS SVM Regressor.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nC=1.0: The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\nkernel=\"rbf\": linear, poly, rbf, sigmoid are supported.\ndegree=3: Degree of polynomial kernel function.\ngamma=\"scale\"\nauto: gamma will be set to 1 / n_features\nscale: gamma will be set to 1 / (n_features * var(X))\ncoef0=0.0: Independent term in kernel function, only signifficant for poly and sigmoid.\ntol=0.001: Tolerance for stopping criterion.\ncache_size=1024.0: Size of the cache during training in MiB.\nmax_iter=-1: Limit the number of outer iterations in the solver. If -1 (default) then max_iter=100*n_samples.\nnochange_steps=1000: Stop training if a 1e-3*tol difference isn't seen in nochange_steps steps.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = SVR()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.LinearSVR","page":"cuMl","title":"RAPIDS.LinearSVR","text":"LinearSVR\n\nA model type for constructing a linear svr, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearSVR = @load LinearSVR pkg=cuML Regression Methods\n\nDo model = LinearSVR() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearSVR(penalty=...).\n\nSVR is a wrapper for the RAPIDS Linear SVM Regressor.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\npenalty=\"l2: l1 (Lasso) or l2 (Ridge) penalty.\nloss=\"epsilon_insensitive\": The loss term of the target function.\nfit_intercept=true: If true, the model tries to correct for the global mean of y.                        If false, the model expects that you have centered the data.\npenalized_intercept=true: When true, the bias term is treated the same way as other features.\nmax_iter=1000: Maximum number of iterations for the underlying solver.\nlinesearch_max_iter=1000: Maximum number of linesearch (inner loop) iterations for the underlying (QN) solver.\nlbfgs_memory=5: Number of vectors approximating the hessian for the underlying QN solver (l-bfgs).\nC=1.0: The constant scaling factor of the loss term in the target formula F(X, y) = penalty(X) + C * loss(X, y).\ngrad_tol=0.0001: The threshold on the gradient for the underlying QN solver.\nchange_tol=0.00001: The threshold on the function change for the underlying QN solver.\ntol=nothing: Tolerance for stopping criterion.\nepsilon=0.0: The epsilon-sensitivity parameter for the SVR loss function.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = LinearSVR()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.KNeighborsRegressor","page":"cuMl","title":"RAPIDS.KNeighborsRegressor","text":"KNeighborsRegressor\n\nA model type for constructing a k neighbors regressor, based on cuML Regression Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKNeighborsRegressor = @load KNeighborsRegressor pkg=cuML Regression Methods\n\nDo model = KNeighborsRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KNeighborsRegressor(n_neighbors=...).\n\nKNeighborsRegressor is a wrapper for the RAPIDS K-Nearest Neighbors Regressor.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\ny: is an AbstractVector continuous target.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_neighbors=5: Default number of neighbors to query.\nalgorithm=\"auto\": The query algorithm to use. \nmetric=\"euclidean\": Distance metric to use.\nweights=\"uniform\": Sample weights to use. Currently, only the uniform strategy is supported.\nverbose=false: Sets logging level.\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given   features Xnew having the same scitype as X above. Predictions   are class assignments. \n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures: the names of the features encountered in training.\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = rand(100)\n\nmodel = KNeighborsRegressor()\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#Dimensionality-Reduction","page":"cuMl","title":"Dimensionality Reduction","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"PCA\nIncrementalPCA\nTruncatedSVD\nUMAP\nTSNE\nGaussianRandomProjection","category":"page"},{"location":"cuml/#RAPIDS.PCA","page":"cuMl","title":"RAPIDS.PCA","text":"PCA\n\nA model type for constructing a pca, based on cuML Dimensionality Reduction and Manifold Learning Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nPCA = @load PCA pkg=cuML Dimensionality Reduction and Manifold Learning Methods\n\nDo model = PCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PCA(copy=...).\n\nPCA is a wrapper for the RAPIDS PCA.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\ncopy=false: If True, then copies data then removes mean from data.\niterated_power=15: Used in Jacobi solver. The more iterations, the more accurate, but slower.\nn_components=nothing: The number of top K singular vectors / values you want.\nrandom_state=nothing: Seed for the random number generator.\nsvd_solver=\"full: \nfull: eigendecomposition of the covariance matrix then discards components.\njacobi: much faster as it iteratively corrects, but is less accurate.\ntol=1e-7: Convergence tolerance for jacobi. \nwhiten=false: If True, de-correlates the components.\nverbose=false: Sets logging level.\n\nOperations\n\ntansform(mach, Xnew)\ninverse_transform(mach, Xtrans)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = PCA(n_components=2)\nmach = machine(model, X)\nfit!(mach)\nX_trans = transform(mach, X)\ninverse_transform(mach, X)\n\nprintln(mach.fitresult.components_)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.IncrementalPCA","page":"cuMl","title":"RAPIDS.IncrementalPCA","text":"IncrementalPCA\n\nA model type for constructing a incremental pca, based on cuML Dimensionality Reduction and Manifold Learning Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nIncrementalPCA = @load IncrementalPCA pkg=cuML Dimensionality Reduction and Manifold Learning Methods\n\nDo model = IncrementalPCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in IncrementalPCA(copy=...).\n\nIncrementalPCA is a wrapper for the RAPIDS IncrementalPCA.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\ncopy=false: If True, then copies data then removes mean from data.\nn_components=nothing: The number of top K singular vectors / values you want.\ntol=1e-7: Convergence tolerance for jacobi. \nwhiten=false: If True, de-correlates the components.\nbatch_size=nothing: The number of samples to use for each batch. Only used when calling fit.\nverbose=false: Sets logging level.\n\nOperations\n\ntansform(mach, Xnew)\ninverse_transform(mach, Xtrans)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = IncrementalPCA(n_components=2)\nmach = machine(model, X)\nfit!(mach)\nX_trans = transform(mach, X)\ninverse_transform(mach, X)\n\nprintln(mach.fitresult.components_)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.TruncatedSVD","page":"cuMl","title":"RAPIDS.TruncatedSVD","text":"TruncatedSVD\n\nA model type for constructing a truncated svd, based on cuML Dimensionality Reduction and Manifold Learning Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTruncatedSVD = @load TruncatedSVD pkg=cuML Dimensionality Reduction and Manifold Learning Methods\n\nDo model = TruncatedSVD() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TruncatedSVD(n_components=...).\n\nTruncatedSVD is a wrapper for the RAPIDS TruncatedSVD.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nn_components=nothing: The number of top K singular vectors / values you want.\nn_iter=15: The number of top K singular vectors / values you want.\nrandom_state=nothing: Seed for the random number generator.\ntol=1e-7: Convergence tolerance for jacobi.\nverbose=false: Sets logging level.\n\nOperations\n\ntansform(mach, Xnew)\ninverse_transform(mach, Xtrans)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = TruncatedSVD(n_components=2)\nmach = machine(model, X)\nfit!(mach)\nX_trans = transform(mach, X)\ninverse_transform(mach, X_trans)\n\nprintln(mach.fitresult.components_)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.UMAP","page":"cuMl","title":"RAPIDS.UMAP","text":"UMAP\n\nA model type for constructing a umap, based on cuML Dimensionality Reduction and Manifold Learning Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUMAP = @load UMAP pkg=cuML Dimensionality Reduction and Manifold Learning Methods\n\nDo model = UMAP() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UMAP(n_neighbors=...).\n\nUMAP is a wrapper for the RAPIDS UMAP.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nn_neighbors=15: The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\nn_components=2: The dimension of the space to embed into.\nmetric=\"euclidean\": l1, cityblock, taxicab, manhattan, euclidean, l2, sqeuclidean, canberra, minkowski, chebyshev, linf, cosine, correlation, hellinger, hamming, jaccard\nn_epochs=nothing: The number of training epochs to be used in optimizing the low dimensional embedding. \nlearning_rate=1.0: The initial learning rate for the embedding optimization.\ninit=\"spectral\": How to initialize the low dimensional embedding. \nspectral: use a spectral embedding of the fuzzy 1-skeleton.\nrandom: assign initial embedding positions at random.\nmin_dist=0.1: The effective minimum distance between embedded points.\nspread=1.0: The effective scale of embedded points.\nset_op_mix_ratio=1.0: Interpolate between (fuzzy) union and intersection as the set operation used to combine local fuzzy simplicial sets to obtain a global fuzzy simplicial sets.\nlocal_connectivity=1: The local connectivity required - i.e. the number of nearest neighbors that should be assumed to be connected at a local level. \nrepulsion_strength=1.0: Weighting applied to negative samples in low dimensional embedding optimization.\nnegative_sample_rate=5: The number of negative samples to select per positive sample in the optimization process.\ntransform_queue_size=4.0: For transform operations (embedding new points using a trained model this will control how aggressively to search for nearest neighbors.\na=nothing: More specific parameters controlling the embedding.\nb=nothing: More specific parameters controlling the embedding.\nhash_input=false: Hash input, so exact embeddings are return when transform is called on the same data upon which the model was trained.\nrandom_state=nothing: Seed for the random number generator.`\ncallback=nothing: Used to intercept the internal state of embeddings while they are being trained.\nverbose=false: Sets logging level.\n\nOperations\n\ntansform(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = UMAP(n_components=2)\nmach = machine(model, X)\nfit!(mach)\nX_trans = transform(mach, X)\n\nprintln(mach.fitresult.embedding_)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.TSNE","page":"cuMl","title":"RAPIDS.TSNE","text":"TSNE\n\nA model type for constructing a tsne, based on cuML Dimensionality Reduction and Manifold Learning Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTSNE = @load TSNE pkg=cuML Dimensionality Reduction and Manifold Learning Methods\n\nDo model = TSNE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TSNE(n_components=...).\n\nTSNE is a wrapper for the RAPIDS TSNE.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nn_components=2: The output dimensionality size. Currently only 2 is supported.\nperplexity=30.0\nearly_exaggeration=12.0: Space between clusters.\nlate_exaggeration=1.0: Space between clusters. It may be beneficial to increase this slightly to improve cluster separation.\nlearning_rate=200.0: The learning rate usually between (10, 1000).\nn_iter: Number of epochs. The more epochs, the more stable/accurate the final embedding.\nn_iter_without_progress=300: Currently unused. When the KL Divergence becomes too small after some iterations, terminate t-SNE early.\nmin_grad_norm=1e-7: The minimum gradient norm for when t-SNE will terminate early. Used in the exact and fft algorithms.\nmetric=\"euclidean\": l1, cityblock, manhattan, euclidean, l2, sqeuclidean, minkowski, chebyshev, cosine, correlation\ninit=\"random\": Only random is supported.\nmethod=\"fft\": barnes_hut and fft are fast approximations. exact is more accurate but slower.\nangle=0.5: Valid values are between 0.0 and 1.0, which trade off speed and accuracy, respectively.\nlearning_rate_method=\"adaptive\": adaptive or none.\nn_neighbors=90: The number of datapoints you want to use in the attractive forces.\nperplexity_max_iter=100: The number of epochs the best gaussian bands are found for.\nexaggeration_iter=250: To promote the growth of clusters, set this higher.\npre_momentum=0.5: During the exaggeration iteration, more forcefully apply gradients.\npost_momentum=0.8: During the late phases, less forcefully apply gradients.\nsquare_distances=true: Whether TSNE should square the distance values.\nrandom_state=nothing: Seed for the random number generator.\nverbose=false: Sets logging level.\n\nOperations\n\ntansform(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = TSNE(n_components=2)\nmach = machine(model, X)\nfit!(mach)\nX_trans = transform(mach, X)\n\nprintln(mach.fitresult.kl_divergence_)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.GaussianRandomProjection","page":"cuMl","title":"RAPIDS.GaussianRandomProjection","text":"GaussianRandomProjection\n\nA model type for constructing a gaussian random projection, based on cuML Dimensionality Reduction and Manifold Learning Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGaussianRandomProjection = @load GaussianRandomProjection pkg=cuML Dimensionality Reduction and Manifold Learning Methods\n\nDo model = GaussianRandomProjection() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in GaussianRandomProjection(n_components=...).\n\nGaussianRandomProjection is a wrapper for the RAPIDS GaussianRandomProjection.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nn_components=\"auto\": Dimensionality of the target projection space. \neps=0.1: Error tolerance during projection.\nrandom_state=nothing: Seed for the random number generator.\nverbose=false: Sets logging level.\n\nOperations\n\ntansform(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\n\nmodel = GaussianRandomProjection(n_components=2)\nmach = machine(model, X)\nfit!(mach)\nX_trans = transform(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#Time-Series","page":"cuMl","title":"Time Series","text":"","category":"section"},{"location":"cuml/","page":"cuMl","title":"cuMl","text":"ExponentialSmoothing\nARIMA","category":"page"},{"location":"cuml/#RAPIDS.ExponentialSmoothing","page":"cuMl","title":"RAPIDS.ExponentialSmoothing","text":"ExponentialSmoothing\n\nA model type for constructing a exponential smoothing, based on cuML Time Series Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nExponentialSmoothing = @load ExponentialSmoothing pkg=cuML Time Series Methods\n\nDo model = ExponentialSmoothing() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ExponentialSmoothing(seasonal=...).\n\nExponentialSmoothing is a wrapper for the RAPIDS HoltWinters time series analysis model.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\nseasonal=\"additive\": Whether the seasonal trend should be calculated additively or multiplicatively.\nseasonal_periods=2: The seasonality of the data (how often it repeats). For monthly data this should be 12, for weekly data, this should be 7.\nstart_periods=2: Number of seasons to be used for seasonal seed values.\neps=2.24e-3: The accuracy to which gradient descent should achieve.\nverbose=false: Sets logging level.\n\nOperations\n\nforecast(mach, n_timesteps): return a forecasted series of n_timesteps\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = ExponentialSmoothing()\nmach = machine(model, X)\nfit!(mach)\npreds = forecast(mach, 4)\n\n\n\n\n\n","category":"type"},{"location":"cuml/#RAPIDS.ARIMA","page":"cuMl","title":"RAPIDS.ARIMA","text":"ARIMA\n\nA model type for constructing a arima, based on cuML Time Series Methods.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nARIMA = @load ARIMA pkg=cuML Time Series Methods\n\nDo model = ARIMA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ARIMA(exog=...).\n\nARIMA is a wrapper for the RAPIDS batched ARIMA model for in- and out-of-sample time-series prediction, with support for seasonality (SARIMA).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y)\n\nwhere\n\nX: any table or array of input features (eg, a DataFrame) whose columns   each have one of the following element scitypes: Continuous\n\nHyper-parameters\n\norder=(1,1,1): The ARIMA order (p, d, q) of the model.\nseasonal_order=(0,0,0,0):The seasonal ARIMA order (P, D, Q, s) of the model.\nexog=nothing: Exogenous variables, assumed to have each time series in columns, such that variables associated with a same batch member are adjacent. \nThis must be a PyArray\nverbose=false: Sets logging level.\nfit_intercept=true: If True, include a constant trend mu in the model.\nsimple_differencing=true: If True, the data is differenced before being passed to the Kalman filter.                            -If False, differencing is part of the state-space model.\nverbose=false: Sets logging level.\n\nOperations\n\nforecast(mach, n_timesteps): return a forecasted series of n_timesteps\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: the trained model object created by the RAPIDS.jl package\n\nReport\n\nThe fields of report(mach) are:\n\nExamples\n\nusing RAPIDS\nusing MLJBase\n\nX = rand(100, 5)\ny = [repeat([0], 50)..., repeat([1], 50)...]\n\nmodel = ARIMA()\nmach = machine(model, X)\nfit!(mach)\npreds = forecast(mach, 4)\n\n\n\n\n\n","category":"type"}]
}
